x0 <- c(-3, -1)
gamma <- 0.085
GD <- function(x0, gamma) {
'x0 needs to be vector of length of 2'
'gamma needs to be a number'
x1 <- x0 -  gamma* c( 2*x0[1] + x0[2] - 5, x0[1] + 20*x0[2] - 3)
while( sum(x1 == x0) != 2){
x0 <- x1
#print(x0)
x1 <- x0 - gamma * c( 2*x0[1] + x0[2] - 5, x0[1] + 20*x0[2] - 3)
ifelse(x0 == x1, break, next)
}
return(x1)
}
GD(x0, gamma)
rm(list=ls())
library(ISLR)
install.packages("ISLR")
library(ISLR)
set.seed(1)
str(Auto)
dim(Auto)
# Dividing our sample
train = sample(392,196)  #without replacement
# Running our regression with training data
lm.fit = lm( mpg ~ horsepower, data=Auto , subset=train )
lm.fit$coefficients
attach(Auto) #Objects in the database can be accessed by simply giving their names
# MSE (mean squared error)
mean( ( mpg - predict(lm.fit,Auto) )[-train]^2 )
lm.fit2=lm( mpg ~ poly(horsepower,2), data=Auto , subset=train )
mean( ( mpg - predict(lm.fit2,Auto) )[-train]^2 )
# Analogous but with polynomial of degree 3
lm.fit3=lm( mpg ~ poly(horsepower,3), data=Auto ,subset=train )
mean( ( mpg-predict(lm.fit3,Auto) )[-train]^2 )
# A different random selection
set.seed(2)
train = sample(392,196)
lm.fit=lm(mpg~horsepower,subset=train)
mean((mpg-predict(lm.fit,Auto))[-train]^2)
lm.fit2=lm(mpg~poly(horsepower,2),data=Auto,subset=train)
mean((mpg-predict(lm.fit2,Auto))[-train]^2)
lm.fit3=lm(mpg~poly(horsepower,3),data=Auto,subset=train)
mean((mpg-predict(lm.fit3,Auto))[-train]^2)
glm.fit = glm( mpg ~ horsepower, data=Auto )
coef(glm.fit)
lm.fit=lm( mpg ~ horsepower, data=Auto )
coef(lm.fit)
install.packages("boot")
library(boot)
glm.fit = glm( mpg ~ horsepower , data=Auto )
#It gives us the LOOCV error by default
cv.err = cv.glm( Auto , glm.fit )
#Delta delivers cross validation error
#delta[1] for LOOCV and delta[2] for K-Groups
cv.err$delta
cv.err$delta[1]
cv.error = rep(0,5)
cv.error = rep(0,5)
for (i in 1:5){
glm.fit = glm( mpg ~ poly(horsepower,i), data=Auto )
cv.error[i] = cv.glm( Auto , glm.fit )$delta[1]
}
cv.error
install.packages( "gbm" )
install.packages( "tree" )
install.packages( "ISLR2" )
install.packages( "BART" )
install.packages( "randomForest" )
library(gbm)
library(BART)
library(randomForest)
rm( list = ls() )
library(tree)
library(ISLR2)
attach(Carseats)
View(Carseats)
# We record the continous variable Sales as binary variable
# If sales are greater than 8 we labeled as Yes otherwise No
High <- factor( ifelse( Sales <= 8, "No", "Yes" ) )
# We add these variable to the dataframe
Carseats <- data.frame(Carseats , High)
View(Carseats)
# We fit a classification tree in order to predict
# High variable using all variables but Sales
tree.carseats <- tree(High ∼ . - Sales, Carseats)
View(tree.carseats)
# We inspect the variables that were used as internal nodes
# Explore the number of terminal nodes
# The training error rate
tree.carseats.sum <- summary(tree.carseats)
tree.carseats.sum
attributes(tree.carseats.sum)
# See variables used
used_vars <- as.character(tree.carseats.sum$used)
used_vars
# See all available variables
available_vars <- attr( tree.carseats$terms, "term.labels" )
available_vars
# Variables not used
setdiff( available_vars, used_vars )
# Get the number of terminal nodes
length( unique( tree.carseats$where ) )
# Get Residual Mean deviance
deviance <- tree.carseats.sum$dev
# Count total nodes
n_obs <- nrow( Carseats )
deviance / (n_obs - 27)  #27 # de nodos terminales
# We can see tree structure
# x11(width = 50, height = 20)
png( "C://Q_lab//2023-0//ML//PD4//tree1.png", width = 1200, height = 700, )
# We can see tree structure
# x11(width = 50, height = 20)
png( "C:/Users/MEMO 123/Desktop/kto/Vacaciones/Qlab/Machine learning/PDs/PD5",
width = 1200, height = 700, )
# We can see tree structure
# x11(width = 50, height = 20)
png("C:/Users/MEMO 123/Desktop/kto/Vacaciones/Qlab/Machine learning/PDs/PD5",
width = 1200, height = 700, )
plot( tree.carseats )
# Add node labels
# Pretty is used to show the category names
# of any qualitative predictors
text( tree.carseats , pretty = 0, cex=0.8 )
title( main = "Unpruned Classification Tree")
# 3. Close the file
dev.off()
# See the tree in text form
tree.carseats
set.seed (2)
# Get train data
train <- sample( 1: nrow(Carseats), 200 )
# Get test data
Carseats.test <- Carseats[-train, ]
High.test <- High[-train]
# Fit tree using train data
tree.carseats <- tree(High ∼ . - Sales, Carseats,
subset = train)
# We need to add "class" to prediction
# to predict labels
# Get prediction for test data
tree.pred <- predict(tree.carseats , Carseats.test ,
type = "class")
# Confusion matrix
table(tree.pred , High.test)
(104 + 50) / 200
set.seed (7)
cv.carseats <- cv.tree(tree.carseats , FUN = prune.misclass)
View(cv.carseats)
names(cv.carseats)
# Number of terminal nodes
cv.carseats$size
# Number of cross validation
cv.carseats$dev
# Cost Complexity Parameter
cv.carseats$k
# Plot N cv~ Terminal Nodes
plot( cv.carseats$size , cv.carseats$dev, type = "b")
# Selecting a subtree
# We get the nine-node tree which is the best
# why is the best? Because is the tree with only 74 cross validation errors
prune.carseats <- prune.misclass(tree.carseats , best = 9)
plot(prune.carseats)
text(prune.carseats , pretty = 0)
# We test this selected tree
tree.pred <- predict(prune.carseats , Carseats.test ,
type = "class")
table(tree.pred , High.test)
(97 + 58) / 200
# If we choos a different tree, we get lower classification accuracy
prune.carseats <- prune.misclass(tree.carseats , best = 14)
plot(prune.carseats)
text(prune.carseats , pretty = 0)
tree.pred <- predict(prune.carseats , Carseats.test ,
type = "class")
table(tree.pred , High.test)
(102 + 52) / 200
# Fit model with train data
set.seed (1)
train <- sample (1: nrow(Boston), nrow(Boston) / 2)
tree.boston <- tree(medv ∼ ., Boston , subset = train)
tree.boston.sum
tree.boston.sum <- summary(tree.boston)
tree.boston.sum
# Only four of the variables have been used
# See variables used
used_vars <- as.character(tree.boston.sum$used)
used_vars
# See all available variables
available_vars <- attr( tree.boston$terms, "term.labels" )
available_vars
# Variables not used
setdiff( available_vars, used_vars )
# See the tree
plot(tree.boston)
text(tree.boston , pretty = 0)
# Prune the tree using Cross Validation
cv.boston <- cv.tree(tree.boston)
plot(cv.boston$size , cv.boston$dev, type = "b")
# Select the best tree
prune.boston <- prune.tree(tree.boston , best = 5)
plot(prune.boston)
text(prune.boston , pretty = 0)
# Evaluate the selected tree
yhat <- predict(tree.boston , newdata = Boston[-train , ])
boston.test <- Boston[-train, "medv"]
plot(yhat , boston.test)
abline (0, 1)
mean (( yhat - boston.test)^2)
# Select the best tree
prune.boston <- prune.tree(tree.boston , best = 7)
plot(prune.boston)
text(prune.boston , pretty = 0)
# Evaluate the selected tree
yhat <- predict(tree.boston , newdata = Boston[-train , ])
boston.test <- Boston[-train, "medv"]
plot(yhat , boston.test)
abline (0, 1)
mean (( yhat - boston.test)^2)
set.seed (1)
# We use Boston Data
library(randomForest)
View(Boston)
dim(Boston)
#mtry = 12 -> total de vars
#mtry = numero de vars para cada arbol
bag.boston <- randomForest(medv ∼ ., data = Boston ,
subset = train , mtry = 12, importance = TRUE)
# mtry indicates that all the predictos should be used
bag.boston
# See peroformance
yhat.bag <- predict(bag.boston , newdata = Boston[-train , ])
plot(yhat.bag , boston.test)
abline (0, 1)
mean (( yhat.bag - boston.test)^2)
# Increase the number of trees to 25
bag.boston <- randomForest(medv ∼ ., data = Boston ,
subset = train ,
mtry = 12, ntree = 25)
yhat.bag <- predict(bag.boston , newdata = Boston[-train , ])
mean (( yhat.bag - boston.test)^2)
##############################
#   RF m < p           #
#############################
set.seed (1)
# by default m = p/3
rf.boston <- randomForest(medv ∼ ., data = Boston ,
subset = train , mtry = 6, importance = TRUE)
yhat.rf <- predict(rf.boston, newdata = Boston[-train , ])
mean (( yhat.rf - boston.test)^2)
# MSE 19.63718
# Improvement over bagging
# See the importance of variables
importance(rf.boston)
# Plot both variables
varImpPlot(rf.boston)
knitr::opts_chunk$set(echo = TRUE)
setwd("C:/Users/MEMO 123/Desktop/DashboardPension65")
library(flexdashboard)
readRDS("choro2013.rds")
readRDS("choro2023.rds")
knitr::include_graphics("RedVester.png")
readRDS("scatter2013.rds")
readRDS("scatter2022.rds")
knitr::include_graphics("Grafico_lineas.png")
knitr::include_graphics("Grafico_lineas.png")
library(flexdashboard)
knitr::include_graphics("Grafico_lineas.png")
knitr::include_graphics("Grafico_lineas.png")
include_graphics("C:/Users/MEMO 123/Desktop/DashboardPension65/Grafico_lineas.png")
knitr::include_graphics("C:/Users/MEMO 123/Desktop/DashboardPension65/Grafico_lineas.png")
setwd("C:/Users/MEMO 123/Desktop/kto/10/ASIE/Mapas/DashboardPension65")
library(flexdashboard)
readRDS("choro2013.rds")
readRDS("choro2023.rds")
knitr::include_graphics("RedVester.png")
readRDS("scatter2013.rds")
readRDS("scatter2022.rds")
knitr::include_graphics("C:/Users/MEMO 123/Desktop/DashboardPension65/Grafico_lineas.png")
knitr::include_graphics("Grafico_lineas.png")
library(flexdashboard)
setwd("C:/Users/MEMO 123/Desktop/kto/10/ASIE/Mapas/DashboardPension65")
readRDS("choro2013.rds")
readRDS("choro2022.rds")
readRDS("choroDif.rds")
knitr::include_graphics("RedVesterMejorada.png")
readRDS("scatter2013.rds")
readRDS("scatter2022.rds")
readRDS("scatter2022.rds")
knitr::include_graphics("Grafico_lineas_Pension65_v2.png")
setwd("C:/Users/MEMO 123/Desktop/kto/10/ASIE/Mapas/DashboardPension65")
library(flexdashboard)
readRDS("choro2013.rds")
readRDS("choro2022.rds")
readRDS("choroDif.rds")
knitr::include_graphics("RedVesterMejorada.png")
readRDS("cuadrantes2013.rds")
readRDS("cuadrantes2013.rds")
readRDS("cuadrantes2022.rds")
knitr::include_graphics("Grafico_lineas_Pension65_v2.png")
library(flexdashboard)
setwd("C:/Users/MEMO 123/Desktop/kto/10/ASIE/Mapas/DashboardPension65")
library(flexdashboard)
readRDS("choro2013.rds")
readRDS("choro2022.rds")
readRDS("choroDif.rds")
knitr::include_graphics("RedVesterMejorada.png")
knitr::include_graphics("RedVesterMejorada.png")
readRDS("cuadrantes2013.rds")
readRDS("cuadrantes2022.rds")
readRDS("cuadrantes2022.rds")
knitr::include_graphics("Grafico_lineas_Pension65_v2.png")
setwd("C:/Users/MEMO 123/Desktop/kto/10/ASIE/Mapas/DashboardPension65")
library(flexdashboard)
knitr::include_graphics("RedVesterMejorada.png")
readRDS("choro2013.rds")
readRDS("choro2022.rds")
readRDS("cuadrantes2013.rds")
readRDS("cuadrantes2022.rds")
setwd("C:/Users/MEMO 123/Desktop/kto/10/ASIE/Mapas/DashboardPension65")
library(flexdashboard)
readRDS("choro2013.rds")
readRDS("choro2022.rds")
readRDS("choroDif.rds")
knitr::include_graphics("RedVesterMejorada.png")
readRDS("Mapacuadrantes2013.rds")
readRDS("Mapacuadrantes2022.rds")
knitr::include_graphics("Grafico_lineas_Pension65_v3.png")
